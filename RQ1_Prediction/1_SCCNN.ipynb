{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ff0e31",
   "metadata": {},
   "source": [
    "# Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.backend import expand_dims\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Reshape, MaxPooling2D, Flatten, Concatenate, Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import keras\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fafcf2e",
   "metadata": {},
   "source": [
    "# Variables & Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ce0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 20\n",
    "MAX_NUM_WORDS = 250000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f24c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_seq_len(data_df):\n",
    "    titles = data_df['title']\n",
    "    title_lengths = (titles.str.count(' ')+1).fillna(0).astype(np.int)\n",
    "    max_title_len = title_lengths.quantile(0.95, interpolation='higher')\n",
    "    print(max_title_len)\n",
    "\n",
    "    descriptions = data_df['description']\n",
    "    desc_lengths = (descriptions.str.count(' ')+1).fillna(0).astype(np.int)\n",
    "    max_desc_len = desc_lengths.quantile(0.95, interpolation='higher')\n",
    "    print(max_desc_len)\n",
    "    \n",
    "    return max_title_len + max_desc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b942d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = \"apache\"\n",
    "## 'apache', 243k, \n",
    "# 'mojang', 200k, \n",
    "## 'redhat', 106k\n",
    "# 'jira', 98k\n",
    "\n",
    "# 'mongodb', 38k\n",
    "# 'qt', 36k\n",
    "\n",
    "# 'sakai', 19k\n",
    "# 'hyperledger', 16k \n",
    "# 'mariadb', 15k\n",
    "# 'spring', 14k\n",
    "# 'jiraecosystem', 11k\n",
    "\n",
    "# 'sonatype', 4k, small\n",
    "# 'jfrog', 3k, small\n",
    "#'inteldaos', 3k too small DOES NOT WORK without properties\n",
    "\n",
    "# 'secondlife', 630 too small\n",
    "## 'mindville', 44 too small DOES NOT WORK\n",
    "\n",
    "NL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d075377",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict={'Backports': 'Backport', \n",
    "           \n",
    "    'Blocked': 'Block',\n",
    "    'Blocker': 'Block',\n",
    "    'Blocks': 'Block',\n",
    "           \n",
    "    'Bonfire Testing': 'Bonfire Testing', \n",
    "    'Bonfire testing': 'Bonfire Testing', \n",
    "    'Git Code Review': 'Bonfire Testing', \n",
    "    'Testing': 'Bonfire Testing',\n",
    "           \n",
    "    'Causality': 'Cause', \n",
    "    'Cause': 'Cause',\n",
    "    'Caused': 'Cause', \n",
    "    'Problem/Incident': 'Cause',\n",
    "           \n",
    "    'Child-Issue': 'Parent-Child', \n",
    "    'Parent Feature': 'Parent-Child',\n",
    "    'Parent/Child': 'Parent-Child',\n",
    "    'multi-level hierarchy [GANTT]': 'Parent-Child',\n",
    "    'Parent-Relation': 'Parent-Child',\n",
    "           \n",
    "    'Cloners': 'Clone', \n",
    "    'Cloners (old)': 'Clone', \n",
    "           \n",
    "    'Collection': 'Incorporate', \n",
    "    'Container': 'Incorporate',\n",
    "    'Contains(WBSGantt)': 'Incorporate', \n",
    "    'Incorporate': 'Incorporate', \n",
    "    'Incorporates': 'Incorporate', \n",
    "    'Part': 'Incorporate',\n",
    "    'PartOf': 'Incorporate',\n",
    "    'Superset': 'Incorporate', \n",
    "           \n",
    "    'Completes': 'Fix', \n",
    "    'Fixes': 'Fix',\n",
    "    'Resolve': 'Fix',\n",
    "           \n",
    "    'Depend': 'Depend', \n",
    "    'Dependency': 'Depend', \n",
    "    'Dependent': 'Depend', \n",
    "    'Depends': 'Depend', \n",
    "    'Gantt Dependency': 'Depend',\n",
    "    'dependent': 'Depend',\n",
    "           \n",
    "    'Derived': 'Derive',\n",
    "           \n",
    "    'Detail': 'Detail', \n",
    "           \n",
    "    'Documentation': 'Documented',\n",
    "    'Documented': 'Documented',\n",
    "    \n",
    "    'Duplicate': 'Duplicate',\n",
    "           \n",
    "    'Epic': 'Epic', \n",
    "    'Epic-Relation': 'Epic',\n",
    "    \n",
    "    'Finish-to-Finish link (WBSGantt)': 'finish-finish', \n",
    "    'Gantt End to End': 'finish-finish', \n",
    "    'Gantt: finish-finish': 'finish-finish',\n",
    "    'finish-finish [GANTT]': 'finish-finish', \n",
    "    \n",
    "    'Gantt End to Start': 'finish-start', \n",
    "    'Gantt: finish-start': 'finish-start',\n",
    "    'finish-start [GANTT]': 'finish-start',\n",
    "\n",
    "    'Gantt Start to Start': 'start-start', \n",
    "    \n",
    "    'Gantt: start-finish': 'start-finish', \n",
    "    \n",
    "    'Follows': 'Follow', \n",
    "    'Sequence': 'Follow', \n",
    "    \n",
    "    'Implement': 'Implement', \n",
    "    'Implements': 'Implements', \n",
    "    \n",
    "    'Issue split': 'Split',\n",
    "    'Split': 'Split',\n",
    "    'Work Breakdown': 'Split',\n",
    "    \n",
    "    'Preceded By': 'Precede', \n",
    "    \n",
    "    'Reference': 'Relate',\n",
    "    'Relate': 'Relate',\n",
    "    'Related': 'Relate', \n",
    "    'Relates': 'Relate',\n",
    "    'Relationship': 'Relate',\n",
    "    \n",
    "    'Regression': 'Breaks', \n",
    "    \n",
    "    'Replacement': 'Replace',\n",
    "    \n",
    "    'Required': 'Require', \n",
    "    \n",
    "    'Supercedes': 'Supercede',\n",
    "    'Supersede': 'Supercede',\n",
    "    'Supersession': 'Supercede', \n",
    "    \n",
    "    'Subtask': 'Subtask',\n",
    "    \n",
    "    'Test': 'Test', \n",
    "    'Tested': 'Test',\n",
    "    \n",
    "    'Trigger': 'Trigger', \n",
    "    \n",
    "    'Non-Link': 'Non-Link',\n",
    "          \n",
    "    '1 - Relate': 'Relate',\n",
    "'Subtask': 'Subtask',\n",
    "'5 - Depend':   'Depend',          \n",
    "'3 - Duplicate': 'Duplicate',          \n",
    "'4 - Incorporate': 'Incorporate',        \n",
    "'2 - Cloned': 'Clone',               \n",
    "'6 - Blocks': 'Block',                \n",
    "'7 - Git Code Review': 'Bonfire Testing',\n",
    "          'Verify': 'Verify'}\n",
    "\n",
    "cat_dict = {'Block': 'Causal',\n",
    "    'Bonfire Testing': 'Workflow',\n",
    "    'Breaks': 'Causal',\n",
    "    'Cause': 'Causal',\n",
    "    'Clone': 'General',\n",
    "    'Depend': 'Causal',\n",
    "    'Detail': 'Workflow',\n",
    "    'Documented': 'Workflow',\n",
    "    'Duplicate': 'General',\n",
    "    'Epic': 'Epic',\n",
    "    'Fix': 'Workflow',\n",
    "    'Follow': 'Causal',\n",
    "    'Incorporate': 'Split',\n",
    "    'Parent-Child': 'Split',\n",
    "    'Relate': 'General',\n",
    "    'Replace': 'General',\n",
    "    'Require': 'Causal',\n",
    "    'Split': 'Split',\n",
    "    'Subtask': 'Split',\n",
    "    'Supercede': 'Causal',\n",
    "    'Trigger': 'Workflow',\n",
    "    'finish-start': 'Causal',\n",
    "    'Non-Link': 'Non-Link',\n",
    "    'Verify': 'Workflow'\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b5f6b",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38351a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../../data/processed/links_'+SOURCE.lower()+'.csv'\n",
    "links = pd.read_csv(filename, encoding=\"UTF-8\", low_memory=False, index_col=0, sep=\";\")\n",
    "print(\"Number of Links: \" + str(len(links)))\n",
    "print(\"Feature Size: \"+str(len(list(links.columns.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880067d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "links.linktype.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea555304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Issues & Links\n",
    "filename = '../../data/processed/issues_'+SOURCE.lower()+'.csv'\n",
    "issues = pd.read_csv(filename, encoding=\"UTF-8\", low_memory=False, index_col=['issue_id'], sep=\";\")\n",
    "print(\"Number of Issues: \" + str(len(issues)))\n",
    "print(\"Feature Size: \" + str(len(list(issues.columns.values))))\n",
    "\n",
    "if NL:\n",
    "    filename = '../../data/processed/links_plus_'+SOURCE.lower()+'.csv'\n",
    "    links = pd.read_csv(filename, encoding=\"UTF-8\", low_memory=False, index_col=0, sep=\";\")\n",
    "    print(\"Number of Links: \" + str(len(links)))\n",
    "    print(\"Feature Size: \"+str(len(list(links.columns.values))))\n",
    "else:\n",
    "    filename = '../../data/processed/links_'+SOURCE.lower()+'.csv'\n",
    "    links = pd.read_csv(filename, encoding=\"UTF-8\", low_memory=False, index_col=0, sep=\";\")\n",
    "    print(\"Number of Links: \" + str(len(links)))\n",
    "    print(\"Feature Size: \"+str(len(list(links.columns.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81416b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = calc_max_seq_len(issues)\n",
    "print(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ddf58",
   "metadata": {},
   "source": [
    "## Issue preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5fd023",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = np.load('w2v/text_data_'+SOURCE+'.npy')\n",
    "issues['text_emb']=list(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594de8ff",
   "metadata": {},
   "source": [
    "## Links preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65653009",
   "metadata": {},
   "outputs": [],
   "source": [
    "links.linktype.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd5b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "links['mappedtype'] = links['linktype'].map(type_dict)\n",
    "links.mappedtype.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62007171",
   "metadata": {},
   "outputs": [],
   "source": [
    "links.mappedtype.value_counts()>=len(links)*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5afc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "linktypes = (links.mappedtype.value_counts()>=len(links)*0.01).rename_axis('mappedtype').reset_index(name='valid')\n",
    "valid_types = set(linktypes[linktypes['valid']==True]['mappedtype'])\n",
    "\n",
    "all_data = links[(links[\"mappedtype\"].isin(valid_types))]\n",
    "\n",
    "all_data['label'] = all_data['mappedtype'].factorize()[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e58930",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_df = all_data[['mappedtype', 'label']].drop_duplicates().sort_values('label')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['label', 'mappedtype']].values)\n",
    "\n",
    "print(\"Categories: \"+str(len(id_to_category)))\n",
    "print(\"Categories: \"+str(id_to_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43a580",
   "metadata": {},
   "source": [
    "## Features / Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7846d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CBOW = Word2Vec.load('w2v/'+SOURCE.lower()+'W2V.model')\n",
    "\n",
    "embedding_matrix = np.zeros((len(model_CBOW.wv), EMBEDDING_DIM))\n",
    "for i in range(len(model_CBOW.wv)):\n",
    "    embedding_vector = model_CBOW.wv[model_CBOW.wv.index_to_key[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b12448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_feat_data = issues[['text_emb']]\n",
    "\n",
    "all_data = all_data.merge(issue_feat_data, left_on='issue_id_1', right_on='issue_id')\n",
    "all_data = all_data.merge(issue_feat_data, left_on='issue_id_2', right_on='issue_id', suffixes=('_1', '_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20844077",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity Check\n",
    "all_data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1812d",
   "metadata": {},
   "source": [
    "# Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c98a3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE + \"_LT\"\n",
    "if NL:\n",
    "    SOURCE = SOURCE + \"_plus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcecc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating functions.\")\n",
    "def plot_history(history):\n",
    "#     keys = history.history.keys()\n",
    "    for i in list(history.history)[0:2]:\n",
    "        print(i)\n",
    "        # list all data in history\n",
    "        # summarize history for accuracy\n",
    "        plt.plot(history.history[i])\n",
    "#         plt.plot(history.history['val_'+i])\n",
    "        plt.title('model '+ i)\n",
    "        plt.ylabel(i)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd64f1",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b762086",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Disabling eager execution.\")\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "print(\"Models incoming.\")\n",
    "embedding_layer = Embedding(len(model_CBOW.wv),\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "# from keras.layers import concatenate\n",
    "text_in = Input(shape = (MAX_SEQUENCE_LENGTH,), name = 'Text_Input')\n",
    "text_out = embedding_layer(text_in)\n",
    "\n",
    "text_embedding = Model(inputs = [text_in], outputs = [text_out], name = 'Text_Output')\n",
    "text_embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468acf3c",
   "metadata": {},
   "source": [
    "### SC-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a619500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SCCNN_Model():\n",
    "    text_in = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='Text_Input')\n",
    "\n",
    "    text_out = text_embedding([text_in])\n",
    "    \n",
    "    text_out = expand_dims(text_out, axis=-1)\n",
    "\n",
    "     # A branch\n",
    "    conv_a = Conv2D(filters=100,\n",
    "                    kernel_size=(2,20),\n",
    "                    strides=(1,1),\n",
    "                    activation='relu',\n",
    "                    name=\"BranchA\"\n",
    "                   )(text_out)\n",
    "    \n",
    "    conv_a = BatchNormalization(axis=-1)(conv_a)\n",
    "\n",
    "    conv_a_rs = Reshape((MAX_SEQUENCE_LENGTH-1,100,1))(conv_a)\n",
    "\n",
    "    conv_a_1 = Conv2D(filters = 200,\n",
    "                    kernel_size = (2,100),\n",
    "                    strides=(1,1),\n",
    "                    activation = 'relu',\n",
    "                    name=\"BranchA1\"\n",
    "                    )(conv_a_rs)\n",
    "\n",
    "    pooled_conv_a_1 = MaxPooling2D(pool_size=(conv_a_1.shape[1], 1), padding='valid')(conv_a_1)\n",
    "#     pooled_conv_a_1 = GlobalMaxPooling2D()(conv_a_1)\n",
    "    pooled_conv_a_1 = Flatten()(pooled_conv_a_1)\n",
    "\n",
    "    conv_a_2 = Conv2D(filters = 200,\n",
    "                    kernel_size = (3,100),\n",
    "                    activation = 'relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchA2\"\n",
    "                    )(conv_a_rs)\n",
    "\n",
    "    pooled_conv_a_2 = MaxPooling2D(pool_size=(conv_a_2.shape[1], 1), padding='valid')(conv_a_2)\n",
    "#     pooled_conv_a_2 = GlobalMaxPooling2D()(conv_a_2)\n",
    "    pooled_conv_a_2 = Flatten()(pooled_conv_a_2)\n",
    "\n",
    "    conv_a_3 = Conv2D(filters = 200,\n",
    "                    kernel_size = (4,100),\n",
    "                    activation = 'relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchA3\"\n",
    "                    )(conv_a_rs)\n",
    "\n",
    "    pooled_conv_a_3 = MaxPooling2D(pool_size=(conv_a_3.shape[1], 1), padding='valid')(conv_a_3)\n",
    "#     pooled_conv_a_3 = GlobalMaxPooling2D()(conv_a_3)\n",
    "    pooled_conv_a_3 = Flatten()(pooled_conv_a_3)\n",
    "    \n",
    "    A = Concatenate(axis=-1)([pooled_conv_a_1,pooled_conv_a_2])\n",
    "    A = Concatenate(axis=-1)([A,pooled_conv_a_3])\n",
    "    \n",
    "    # B branch\n",
    "    conv_b = Conv2D(filters=100,\n",
    "                    kernel_size=(3,20),\n",
    "                    activation='relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchB\"\n",
    "                    )(text_out)\n",
    "\n",
    "    conv_b = BatchNormalization(axis=-1)(conv_b)\n",
    "\n",
    "    conv_b_rs = Reshape((MAX_SEQUENCE_LENGTH-2,100,1))(conv_b)\n",
    "\n",
    "    conv_b_1 = Conv2D(filters = 200,\n",
    "                    kernel_size = (2,100),\n",
    "                    activation = 'relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchB1\"\n",
    "                    )(conv_b_rs)\n",
    "\n",
    "    pooled_conv_b_1 = MaxPooling2D(pool_size=(conv_b_1.shape[1], 1), padding='valid')(conv_b_1)\n",
    "#     pooled_conv_b_1 = GlobalMaxPooling2D()(conv_b_1)\n",
    "    pooled_conv_b_1 = Flatten()(pooled_conv_b_1)\n",
    "\n",
    "    conv_b_2 = Conv2D(filters = 200,\n",
    "                    kernel_size = (3,100),\n",
    "                    activation = 'relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchB2\"\n",
    "                    )(conv_b_rs)\n",
    "\n",
    "    pooled_conv_b_2 = MaxPooling2D(pool_size=(conv_b_2.shape[1], 1), padding='valid')(conv_b_2)\n",
    "#     pooled_conv_b_2 = GlobalMaxPooling2D()(conv_b_2)\n",
    "    pooled_conv_b_2 = Flatten()(pooled_conv_b_2)\n",
    "\n",
    "    conv_b_3 = Conv2D(filters = 200,\n",
    "                    kernel_size = (4,100),\n",
    "                    activation = 'relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchB3\"\n",
    "                    )(conv_b_rs)\n",
    "\n",
    "    pooled_conv_b_3 = MaxPooling2D(pool_size=(conv_b_3.shape[1], 1), padding='valid')(conv_b_3)\n",
    "#     pooled_conv_b_3 = GlobalMaxPooling2D()(conv_b_3)\n",
    "    pooled_conv_b_3 = Flatten()(pooled_conv_b_3)\n",
    "    \n",
    "    B = Concatenate(axis=-1)([pooled_conv_b_1,pooled_conv_b_2])\n",
    "    B = Concatenate(axis=-1)([B,pooled_conv_b_3])\n",
    "\n",
    "    # C branch\n",
    "    conv_c = Conv2D(filters=100,\n",
    "                    kernel_size=(4,20),\n",
    "                    activation='relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchC\"\n",
    "                    )(text_out)\n",
    "    conv_c = BatchNormalization(axis=-1)(conv_c)\n",
    "\n",
    "    conv_c_rs = Reshape((MAX_SEQUENCE_LENGTH-3,100,1))(conv_c)\n",
    "\n",
    "    conv_c_1 = Conv2D(filters = 200,\n",
    "                    kernel_size = (2,100),\n",
    "                    activation = 'relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchC1\"\n",
    "                    )(conv_c_rs)\n",
    "\n",
    "\n",
    "    pooled_conv_c_1 = MaxPooling2D(pool_size=(conv_c_1.shape[1], 1), padding='valid')(conv_c_1)\n",
    "#     pooled_conv_c_1 = GlobalMaxPooling2D()(conv_c_1)\n",
    "    pooled_conv_c_1 = Flatten()(pooled_conv_c_1)\n",
    "\n",
    "    conv_c_2 = Conv2D(filters = 200,\n",
    "                    kernel_size = (3,100),\n",
    "                    activation = 'relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchC2\"\n",
    "                    )(conv_c_rs)\n",
    "\n",
    "    pooled_conv_c_2 = MaxPooling2D(pool_size=(conv_c_2.shape[1], 1), padding='valid')(conv_c_2)\n",
    "#     pooled_conv_c_2 = GlobalMaxPooling2D()(conv_c_2)\n",
    "    pooled_conv_c_2 = Flatten()(pooled_conv_c_2)\n",
    "\n",
    "    conv_c_3 = Conv2D(filters = 200,\n",
    "                    kernel_size = (4,100),\n",
    "                    activation = 'relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchC3\"\n",
    "                    )(conv_c_rs)\n",
    "\n",
    "    pooled_conv_c_3 = MaxPooling2D(pool_size=(conv_c_3.shape[1], 1), padding='valid')(conv_c_3)\n",
    "#     pooled_conv_c_3 = GlobalMaxPooling2D()(conv_c_3)\n",
    "    pooled_conv_c_3 = Flatten()(pooled_conv_c_3)\n",
    "    \n",
    "    C = Concatenate(axis=-1)([pooled_conv_c_1,pooled_conv_c_2])\n",
    "    C = Concatenate(axis=-1)([C,pooled_conv_c_3])\n",
    "\n",
    "    conv_concat = Concatenate(axis=-1)([A,B])\n",
    "    conv_concat = Concatenate(axis=-1)([conv_concat,C])\n",
    "\n",
    "    issue_model = Model(inputs = [text_in], outputs = [conv_concat], name = 'SC-CNN_Model')\n",
    "\n",
    "#     issue_model.summary()\n",
    "    return issue_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6339a9",
   "metadata": {},
   "source": [
    "### Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    text_in_a = Input(shape = (MAX_SEQUENCE_LENGTH,), dtype='int32', name=\"Text_issue_a\")\n",
    "    text_in_b = Input(shape = (MAX_SEQUENCE_LENGTH,), dtype='int32', name=\"Text_issue_b\")\n",
    "\n",
    "    issue_model = SCCNN_Model()\n",
    "\n",
    "    encoded_issue_a = issue_model([text_in_a])\n",
    "    encoded_issue_b = issue_model([text_in_b])\n",
    "\n",
    "    merged_vector = Concatenate()([encoded_issue_a, encoded_issue_b])\n",
    "\n",
    "    concat = Dropout(0.5)(merged_vector)\n",
    "    concat = Dense(units = 512, \n",
    "                activation = 'relu',\n",
    "                )(concat)\n",
    "    concat = BatchNormalization(axis=-1)(concat)\n",
    "    \n",
    "    concat = Dropout(0.5)(concat)\n",
    "    concat = Dense(units = 256, \n",
    "                activation = 'relu',\n",
    "                )(concat)\n",
    "    concat = BatchNormalization(axis=-1)(concat)\n",
    "    \n",
    "    concat = Dropout(0.5)(concat)\n",
    "    concat = Dense(units = 128, \n",
    "                activation = 'relu',\n",
    "                )(concat)\n",
    "    concat = BatchNormalization(axis=-1)(concat)\n",
    "    \n",
    "    concat = Dropout(0.5)(concat)\n",
    "    concat = Dense(units = 64, \n",
    "                activation = 'relu',\n",
    "                )(concat)\n",
    "    concat = BatchNormalization(axis=-1)(concat)\n",
    "       \n",
    "    model = Model(inputs=[text_in_a, text_in_b], outputs=concat)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44afb575",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2196b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(all_data, test_size = 0.2, random_state = 9)\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b7340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset):\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=2)\n",
    "    \n",
    "    class_weight = compute_class_weight(\n",
    "                      class_weight='balanced',\n",
    "                      classes=range(len(id_to_category)),\n",
    "                      y=dataset['label']\n",
    "                    )\n",
    "\n",
    "    model = get_model()\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "#                       Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0), \n",
    "                      loss=tfa.losses.TripletSemiHardLoss())\n",
    "\n",
    "    train_issue_1 = dataset['text_emb_1']\n",
    "    train_issue_1 = np.array(train_issue_1.values.tolist())\n",
    "\n",
    "    train_issue_2 = dataset['text_emb_2']\n",
    "    train_issue_2 = np.array(train_issue_2.values.tolist())\n",
    "    \n",
    "    history = model.fit([train_issue_1, train_issue_2], \n",
    "                        y=dataset['label'], \n",
    "                            callbacks=[callback], \n",
    "                            validation_split=0.1, \n",
    "#                         class_weight = dict(enumerate(class_weight)), \n",
    "                        batch_size=128, epochs=64, verbose=2)\n",
    "    \n",
    "    plot_history(history)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4562f9e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "class_model = train_model(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80315854",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_1_train = train['text_emb_1']\n",
    "issue_1_train = np.array(issue_1_train.values.tolist())\n",
    "\n",
    "issue_2_train = train['text_emb_2']\n",
    "issue_2_train = np.array(issue_2_train.values.tolist())\n",
    "\n",
    "results_train = class_model.predict([issue_1_train, issue_2_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42531ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_1_test = test['text_emb_1']\n",
    "issue_1_test = np.array(issue_1_test.values.tolist())\n",
    "\n",
    "issue_2_test = test['text_emb_2']\n",
    "issue_2_test = np.array(issue_2_test.values.tolist())\n",
    "\n",
    "results_test = class_model.predict([issue_1_test, issue_2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(file='embeddings/train_embed_LT_'+SOURCE+'.npy', arr=results_train)\n",
    "np.save(file='embeddings/test_embed_LT_'+SOURCE+'.npy', arr=results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af78e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=25, random_state=0)\n",
    "\n",
    "clf.fit(results_train, train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6666c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = clf.predict_proba(results_test)\n",
    "preds = clf.predict(results_test)\n",
    "\n",
    "test['preds_SVM'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfdc2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['name', 'linktype', 'issue_id_1', 'issue_id_2', 'mappedtype', 'label']].to_csv('embeddings/train_df_LT'+SOURCE+'.csv')\n",
    "test[['name', 'linktype', 'issue_id_1', 'issue_id_2', 'mappedtype', 'label', 'preds_SVM']].to_csv('embeddings/test_df_LT'+SOURCE+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_rep = classification_report(test['label'], test['preds_SVM'], output_dict=True, target_names=category_id_df.mappedtype.to_list())\n",
    "class_rep_df = pd.DataFrame(class_rep).transpose()\n",
    "print(class_rep_df)\n",
    "\n",
    "conf_mat = confusion_matrix(test['preds_SVM'], test['label'])\n",
    "conf_mat_df = pd.DataFrame(conf_mat).transpose()\n",
    "conf_mat_df.rename(index=id_to_category, inplace=True)\n",
    "conf_mat_df.rename(columns=id_to_category, inplace=True)\n",
    "print(conf_mat_df)\n",
    "\n",
    "class_rep_df.to_csv('results/class_rep_LT_'+SOURCE+\"_SCCNN.csv\")\n",
    "conf_mat_df.to_csv('results/conf_mat_LT_'+SOURCE+\"_SCCNN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9546909e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
