{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ff0e31",
   "metadata": {},
   "source": [
    "# Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40f002b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/lueders/LYNX-venv/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "/export/home/lueders/LYNX-venv/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.backend import expand_dims\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Reshape, MaxPooling2D, Flatten, Concatenate, Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import keras\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fafcf2e",
   "metadata": {},
   "source": [
    "# Variables & Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2ce0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 20\n",
    "MAX_NUM_WORDS = 250000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f6a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_seq_len(data_df):\n",
    "    titles = data_df['title']\n",
    "    title_lengths = (titles.str.count(' ')+1).fillna(0).astype(np.int)\n",
    "    max_title_len = title_lengths.quantile(0.95, interpolation='higher')\n",
    "    print(max_title_len)\n",
    "\n",
    "    descriptions = data_df['description']\n",
    "    desc_lengths = (descriptions.str.count(' ')+1).fillna(0).astype(np.int)\n",
    "    max_desc_len = desc_lengths.quantile(0.95, interpolation='higher')\n",
    "    print(max_desc_len)\n",
    "    \n",
    "    return max_title_len + max_desc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b942d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = \"redhat\"\n",
    "# 'apache', 243k, \n",
    "# 'redhat', 106k\n",
    "## 'jira', 98k\n",
    "## 'mojang', 200k\n",
    "\n",
    "## 'mongodb', 38k\n",
    "## 'qt', 36k\n",
    "\n",
    "## 'sakai', 19k\n",
    "## 'hyperledger', 16k \n",
    "## 'mariadb', 15k\n",
    "## 'spring', 14k\n",
    "## 'jiraecosystem', 11k\n",
    "\n",
    "## 'sonatype', 4k, small\n",
    "## 'jfrog', 3k, small\n",
    "## 'inteldaos', 3k too small DOES NOT WORK without properties\n",
    "\n",
    "## 'secondlife', 630 too small\n",
    "## 'mindville', 44 too small DOES NOT WORK\n",
    "\n",
    "NL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d075377",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict={'Backports': 'Backport', \n",
    "           \n",
    "    'Blocked': 'Block',\n",
    "    'Blocker': 'Block',\n",
    "    'Blocks': 'Block',\n",
    "           \n",
    "    'Bonfire Testing': 'Bonfire Testing', \n",
    "    'Bonfire testing': 'Bonfire Testing', \n",
    "    'Git Code Review': 'Bonfire Testing', \n",
    "    'Testing': 'Bonfire Testing',\n",
    "           \n",
    "    'Causality': 'Cause', \n",
    "    'Cause': 'Cause',\n",
    "    'Caused': 'Cause', \n",
    "    'Problem/Incident': 'Cause',\n",
    "           \n",
    "    'Child-Issue': 'Parent-Child', \n",
    "    'Parent Feature': 'Parent-Child',\n",
    "    'Parent/Child': 'Parent-Child',\n",
    "    'multi-level hierarchy [GANTT]': 'Parent-Child',\n",
    "    'Parent-Relation': 'Parent-Child',\n",
    "           \n",
    "    'Cloners': 'Clone', \n",
    "    'Cloners (old)': 'Clone', \n",
    "           \n",
    "    'Collection': 'Incorporate', \n",
    "    'Container': 'Incorporate',\n",
    "    'Contains(WBSGantt)': 'Incorporate', \n",
    "    'Incorporate': 'Incorporate', \n",
    "    'Incorporates': 'Incorporate', \n",
    "    'Part': 'Incorporate',\n",
    "    'PartOf': 'Incorporate',\n",
    "    'Superset': 'Incorporate', \n",
    "           \n",
    "    'Completes': 'Fix', \n",
    "    'Fixes': 'Fix',\n",
    "    'Resolve': 'Fix',\n",
    "           \n",
    "    'Depend': 'Depend', \n",
    "    'Dependency': 'Depend', \n",
    "    'Dependent': 'Depend', \n",
    "    'Depends': 'Depend', \n",
    "    'Gantt Dependency': 'Depend',\n",
    "    'dependent': 'Depend',\n",
    "           \n",
    "    'Derived': 'Derive',\n",
    "           \n",
    "    'Detail': 'Detail', \n",
    "           \n",
    "    'Documentation': 'Documented',\n",
    "    'Documented': 'Documented',\n",
    "    \n",
    "    'Duplicate': 'Duplicate',\n",
    "           \n",
    "    'Epic': 'Epic', \n",
    "    'Epic-Relation': 'Epic',\n",
    "    \n",
    "    'Finish-to-Finish link (WBSGantt)': 'finish-finish', \n",
    "    'Gantt End to End': 'finish-finish', \n",
    "    'Gantt: finish-finish': 'finish-finish',\n",
    "    'finish-finish [GANTT]': 'finish-finish', \n",
    "    \n",
    "    'Gantt End to Start': 'finish-start', \n",
    "    'Gantt: finish-start': 'finish-start',\n",
    "    'finish-start [GANTT]': 'finish-start',\n",
    "\n",
    "    'Gantt Start to Start': 'start-start', \n",
    "    \n",
    "    'Gantt: start-finish': 'start-finish', \n",
    "    \n",
    "    'Follows': 'Follow', \n",
    "    'Sequence': 'Follow', \n",
    "    \n",
    "    'Implement': 'Implement', \n",
    "    'Implements': 'Implements', \n",
    "    \n",
    "    'Issue split': 'Split',\n",
    "    'Split': 'Split',\n",
    "    'Work Breakdown': 'Split',\n",
    "    \n",
    "    'Preceded By': 'Precede', \n",
    "    \n",
    "    'Reference': 'Relate',\n",
    "    'Relate': 'Relate',\n",
    "    'Related': 'Relate', \n",
    "    'Relates': 'Relate',\n",
    "    'Relationship': 'Relate',\n",
    "    \n",
    "    'Regression': 'Breaks', \n",
    "    \n",
    "    'Replacement': 'Replace',\n",
    "    \n",
    "    'Required': 'Require', \n",
    "    \n",
    "    'Supercedes': 'Supercede',\n",
    "    'Supersede': 'Supercede',\n",
    "    'Supersession': 'Supercede', \n",
    "    \n",
    "    'Subtask': 'Subtask',\n",
    "    \n",
    "    'Test': 'Test', \n",
    "    'Tested': 'Test',\n",
    "    \n",
    "    'Trigger': 'Trigger', \n",
    "    \n",
    "    'Non-Link': 'Non-Link',\n",
    "          \n",
    "    '1 - Relate': 'Relate',\n",
    "'Subtask': 'Subtask',\n",
    "'5 - Depend':   'Depend',          \n",
    "'3 - Duplicate': 'Duplicate',          \n",
    "'4 - Incorporate': 'Incorporate',        \n",
    "'2 - Cloned': 'Clone',               \n",
    "'6 - Blocks': 'Block',                \n",
    "'7 - Git Code Review': 'Bonfire Testing',\n",
    "          'Verify': 'Verify'}\n",
    "\n",
    "cat_dict = {'Block': 'Causal',\n",
    "    'Bonfire Testing': 'Workflow',\n",
    "    'Breaks': 'Causal',\n",
    "    'Cause': 'Causal',\n",
    "    'Clone': 'General',\n",
    "    'Depend': 'Causal',\n",
    "    'Detail': 'Workflow',\n",
    "    'Documented': 'Workflow',\n",
    "    'Duplicate': 'General',\n",
    "    'Epic': 'Epic',\n",
    "    'Fix': 'Workflow',\n",
    "    'Follow': 'Causal',\n",
    "    'Incorporate': 'Split',\n",
    "    'Parent-Child': 'Split',\n",
    "    'Relate': 'General',\n",
    "    'Replace': 'General',\n",
    "    'Require': 'Causal',\n",
    "    'Split': 'Split',\n",
    "    'Subtask': 'Split',\n",
    "    'Supercede': 'Causal',\n",
    "    'Trigger': 'Workflow',\n",
    "    'finish-start': 'Causal',\n",
    "    'Non-Link': 'Non-Link',\n",
    "    'Verify': 'Workflow'\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b5f6b",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea555304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Issues: 353000\n",
      "Feature Size: 10\n",
      "Number of Links: 125367\n",
      "Feature Size: 5\n"
     ]
    }
   ],
   "source": [
    "#Loading Issues & Links\n",
    "filename = '../../data/processed/issues_'+SOURCE.lower()+'.csv'\n",
    "issues = pd.read_csv(filename, encoding=\"UTF-8\", low_memory=False, index_col=['issue_id'], sep=\";\")\n",
    "print(\"Number of Issues: \" + str(len(issues)))\n",
    "print(\"Feature Size: \" + str(len(list(issues.columns.values))))\n",
    "\n",
    "if NL:\n",
    "    filename = '../../data/processed/links_plus_'+SOURCE.lower()+'.csv'\n",
    "    links = pd.read_csv(filename, encoding=\"UTF-8\", low_memory=False, index_col=0, sep=\";\")\n",
    "    print(\"Number of Links: \" + str(len(links)))\n",
    "    print(\"Feature Size: \"+str(len(list(links.columns.values))))\n",
    "else:\n",
    "    filename = '../../data/processed/links_'+SOURCE.lower()+'.csv'\n",
    "    links = pd.read_csv(filename, encoding=\"UTF-8\", low_memory=False, index_col=0, sep=\";\")\n",
    "    print(\"Number of Links: \" + str(len(links)))\n",
    "    print(\"Feature Size: \"+str(len(list(links.columns.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92536319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "282\n",
      "297\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = calc_max_seq_len(issues)\n",
    "print(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ddf58",
   "metadata": {},
   "source": [
    "## Issue preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f5fd023",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = np.load('w2v/text_data_'+SOURCE+'.npy')\n",
    "issues['text_emb']=list(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594de8ff",
   "metadata": {},
   "source": [
    "## Links preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65653009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Related                          31006\n",
       "Subtask                          24928\n",
       "Blocks                           18186\n",
       "Cloners                          16969\n",
       "Superset                         10661\n",
       "Duplicate                         5913\n",
       "Non-Link                          5698\n",
       "Sequence                          5129\n",
       "Causality                         3168\n",
       "Cloners (old)                     1504\n",
       "Documentation                      670\n",
       "Parent-Relation                    621\n",
       "multi-level hierarchy [GANTT]      251\n",
       "finish-start [GANTT]               184\n",
       "Account                            166\n",
       "Issue split                        162\n",
       "Gantt: finish-start                 46\n",
       "Gantt: start-finish                 40\n",
       "finish-finish [GANTT]               40\n",
       "Gantt: finish-finish                20\n",
       "start-finish [GANTT]                 4\n",
       "Gantt: start-start                   1\n",
       "Name: linktype, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links.linktype.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82dd5b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Relate           31006\n",
       "Subtask          24928\n",
       "Clone            18473\n",
       "Block            18186\n",
       "Incorporate      10661\n",
       "Duplicate         5913\n",
       "Non-Link          5698\n",
       "Follow            5129\n",
       "Cause             3168\n",
       "Parent-Child       872\n",
       "Documented         670\n",
       "finish-start       230\n",
       "Split              162\n",
       "finish-finish       60\n",
       "start-finish        40\n",
       "Name: mappedtype, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links['mappedtype'] = links['linktype'].map(type_dict)\n",
    "links.mappedtype.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62007171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Relate            True\n",
       "Subtask           True\n",
       "Clone             True\n",
       "Block             True\n",
       "Incorporate       True\n",
       "Duplicate         True\n",
       "Non-Link          True\n",
       "Follow            True\n",
       "Cause             True\n",
       "Parent-Child     False\n",
       "Documented       False\n",
       "finish-start     False\n",
       "Split            False\n",
       "finish-finish    False\n",
       "start-finish     False\n",
       "Name: mappedtype, dtype: bool"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links.mappedtype.value_counts()>=len(links)*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d5afc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2842306/4001403793.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data['label'] = all_data['mappedtype'].factorize()[0].astype(int)\n"
     ]
    }
   ],
   "source": [
    "linktypes = (links.mappedtype.value_counts()>=len(links)*0.01).rename_axis('mappedtype').reset_index(name='valid')\n",
    "valid_types = set(linktypes[linktypes['valid']==True]['mappedtype'])\n",
    "\n",
    "all_data = links[(links[\"mappedtype\"].isin(valid_types))]\n",
    "\n",
    "all_data['label'] = all_data['mappedtype'].factorize()[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3e58930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: 9\n",
      "Categories: {0: 'Block', 1: 'Incorporate', 2: 'Cause', 3: 'Clone', 4: 'Relate', 5: 'Subtask', 6: 'Follow', 7: 'Duplicate', 8: 'Non-Link'}\n"
     ]
    }
   ],
   "source": [
    "category_id_df = all_data[['mappedtype', 'label']].drop_duplicates().sort_values('label')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['label', 'mappedtype']].values)\n",
    "\n",
    "print(\"Categories: \"+str(len(id_to_category)))\n",
    "print(\"Categories: \"+str(id_to_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43a580",
   "metadata": {},
   "source": [
    "## Features / Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab7846d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CBOW = Word2Vec.load('w2v/'+SOURCE+'W2V.model')\n",
    "\n",
    "embedding_matrix = np.zeros((len(model_CBOW.wv), EMBEDDING_DIM))\n",
    "for i in range(len(model_CBOW.wv)):\n",
    "    embedding_vector = model_CBOW.wv[model_CBOW.wv.index_to_key[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b12448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_feat_data = issues[['text_emb']]\n",
    "\n",
    "all_data = all_data.merge(issue_feat_data, left_on='issue_id_1', right_on='issue_id')\n",
    "all_data = all_data.merge(issue_feat_data, left_on='issue_id_2', right_on='issue_id', suffixes=('_1', '_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20844077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name          False\n",
       "linktype      False\n",
       "issue_id_1    False\n",
       "issue_id_2    False\n",
       "issues        False\n",
       "mappedtype    False\n",
       "label         False\n",
       "text_emb_1    False\n",
       "text_emb_2    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sanity Check\n",
    "all_data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1812d",
   "metadata": {},
   "source": [
    "# Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d98ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE + \"_LT\"\n",
    "if NL:\n",
    "    SOURCE = SOURCE + \"_plus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdcecc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating functions.\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating functions.\")\n",
    "def plot_history(history):\n",
    "#     keys = history.history.keys()\n",
    "    for i in list(history.history)[0:2]:\n",
    "        print(i)\n",
    "        # list all data in history\n",
    "        # summarize history for accuracy\n",
    "        plt.plot(history.history[i])\n",
    "#         plt.plot(history.history['val_'+i])\n",
    "        plt.title('model '+ i)\n",
    "        plt.ylabel(i)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd64f1",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b762086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabling eager execution.\n",
      "Models incoming.\n",
      "Model: \"Text_Output\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Text_Input (InputLayer)     [(None, 297)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 297, 20)           12859000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,859,000\n",
      "Trainable params: 0\n",
      "Non-trainable params: 12,859,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Disabling eager execution.\")\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "print(\"Models incoming.\")\n",
    "embedding_layer = Embedding(len(model_CBOW.wv),\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "# from keras.layers import concatenate\n",
    "text_in = Input(shape = (MAX_SEQUENCE_LENGTH,), name = 'Text_Input')\n",
    "text_out = embedding_layer(text_in)\n",
    "\n",
    "text_embedding = Model(inputs = [text_in], outputs = [text_out], name = 'Text_Output')\n",
    "text_embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468acf3c",
   "metadata": {},
   "source": [
    "### SC-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a619500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCCNN_Model():\n",
    "    text_in = Input(shape=(MAX_SEQUENCE_LENGTH, 20, 2), dtype='float32', name='Text_Input')  \n",
    "\n",
    "     # A branch\n",
    "    conv_a = Conv2D(filters=100,\n",
    "                    kernel_size=(1,20),\n",
    "                    strides=(1,1),\n",
    "                    activation='relu',\n",
    "                    name=\"BranchA\"\n",
    "                   )(text_in)\n",
    "    \n",
    "    conv_a = BatchNormalization(axis=-1)(conv_a)\n",
    "\n",
    "    conv_a_rs = Reshape((MAX_SEQUENCE_LENGTH,100,1))(conv_a)\n",
    "\n",
    "    conv_a_1 = Conv2D(filters = 200,\n",
    "                    kernel_size = (1,100),\n",
    "                    strides=(1,1),\n",
    "                    activation = 'relu',\n",
    "                    name=\"BranchA1\"\n",
    "                    )(conv_a_rs)\n",
    "\n",
    "    pooled_conv_a_1 = MaxPooling2D(pool_size=(conv_a_1.shape[1], 1), padding='valid')(conv_a_1)\n",
    "#     pooled_conv_a_1 = GlobalMaxPooling2D()(conv_a_1)\n",
    "    pooled_conv_a_1 = Flatten()(pooled_conv_a_1)\n",
    "\n",
    "    conv_a_2 = Conv2D(filters = 200,\n",
    "                    kernel_size = (2,100),\n",
    "                    activation = 'relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchA2\"\n",
    "                    )(conv_a_rs)\n",
    "\n",
    "    pooled_conv_a_2 = MaxPooling2D(pool_size=(conv_a_2.shape[1], 1), padding='valid')(conv_a_2)\n",
    "#     pooled_conv_a_2 = GlobalMaxPooling2D()(conv_a_2)\n",
    "    pooled_conv_a_2 = Flatten()(pooled_conv_a_2)\n",
    "\n",
    "    conv_a_3 = Conv2D(filters = 200,\n",
    "                    kernel_size = (3,100),\n",
    "                    activation = 'relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchA3\"\n",
    "                    )(conv_a_rs)\n",
    "\n",
    "    pooled_conv_a_3 = MaxPooling2D(pool_size=(conv_a_3.shape[1], 1), padding='valid')(conv_a_3)\n",
    "#     pooled_conv_a_3 = GlobalMaxPooling2D()(conv_a_3)\n",
    "    pooled_conv_a_3 = Flatten()(pooled_conv_a_3)\n",
    "    \n",
    "    A = Concatenate(axis=-1)([pooled_conv_a_1,pooled_conv_a_2])\n",
    "    A = Concatenate(axis=-1)([A,pooled_conv_a_3])\n",
    "    \n",
    "    # B branch\n",
    "    conv_b = Conv2D(filters=100,\n",
    "                    kernel_size=(2,20),\n",
    "                    activation='relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchB\"\n",
    "                    )(text_in)\n",
    "\n",
    "    conv_b = BatchNormalization(axis=-1)(conv_b)\n",
    "\n",
    "    conv_b_rs = Reshape((MAX_SEQUENCE_LENGTH-1,100,1))(conv_b)\n",
    "\n",
    "    conv_b_1 = Conv2D(filters = 200,\n",
    "                    kernel_size = (1,100),\n",
    "                    activation = 'relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchB1\"\n",
    "                    )(conv_b_rs)\n",
    "\n",
    "    pooled_conv_b_1 = MaxPooling2D(pool_size=(conv_b_1.shape[1], 1), padding='valid')(conv_b_1)\n",
    "#     pooled_conv_b_1 = GlobalMaxPooling2D()(conv_b_1)\n",
    "    pooled_conv_b_1 = Flatten()(pooled_conv_b_1)\n",
    "\n",
    "    conv_b_2 = Conv2D(filters = 200,\n",
    "                    kernel_size = (2,100),\n",
    "                    activation = 'relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchB2\"\n",
    "                    )(conv_b_rs)\n",
    "\n",
    "    pooled_conv_b_2 = MaxPooling2D(pool_size=(conv_b_2.shape[1], 1), padding='valid')(conv_b_2)\n",
    "#     pooled_conv_b_2 = GlobalMaxPooling2D()(conv_b_2)\n",
    "    pooled_conv_b_2 = Flatten()(pooled_conv_b_2)\n",
    "\n",
    "    conv_b_3 = Conv2D(filters = 200,\n",
    "                    kernel_size = (3,100),\n",
    "                    activation = 'relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchB3\"\n",
    "                    )(conv_b_rs)\n",
    "\n",
    "    pooled_conv_b_3 = MaxPooling2D(pool_size=(conv_b_3.shape[1], 1), padding='valid')(conv_b_3)\n",
    "#     pooled_conv_b_3 = GlobalMaxPooling2D()(conv_b_3)\n",
    "    pooled_conv_b_3 = Flatten()(pooled_conv_b_3)\n",
    "    \n",
    "    B = Concatenate(axis=-1)([pooled_conv_b_1,pooled_conv_b_2])\n",
    "    B = Concatenate(axis=-1)([B,pooled_conv_b_3])\n",
    "\n",
    "    # C branch\n",
    "    conv_c = Conv2D(filters=100,\n",
    "                    kernel_size=(3,20),\n",
    "                    activation='relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchC\"\n",
    "                    )(text_in)\n",
    "    conv_c = BatchNormalization(axis=-1)(conv_c)\n",
    "\n",
    "    conv_c_rs = Reshape((MAX_SEQUENCE_LENGTH-2,100,1))(conv_c)\n",
    "\n",
    "    conv_c_1 = Conv2D(filters = 200,\n",
    "                    kernel_size = (1,100),\n",
    "                    activation = 'relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchC1\"\n",
    "                    )(conv_c_rs)\n",
    "\n",
    "\n",
    "    pooled_conv_c_1 = MaxPooling2D(pool_size=(conv_c_1.shape[1], 1), padding='valid')(conv_c_1)\n",
    "#     pooled_conv_c_1 = GlobalMaxPooling2D()(conv_c_1)\n",
    "    pooled_conv_c_1 = Flatten()(pooled_conv_c_1)\n",
    "\n",
    "    conv_c_2 = Conv2D(filters = 200,\n",
    "                    kernel_size = (2,100),\n",
    "                    activation = 'relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchC2\"\n",
    "                    )(conv_c_rs)\n",
    "\n",
    "    pooled_conv_c_2 = MaxPooling2D(pool_size=(conv_c_2.shape[1], 1), padding='valid')(conv_c_2)\n",
    "#     pooled_conv_c_2 = GlobalMaxPooling2D()(conv_c_2)\n",
    "    pooled_conv_c_2 = Flatten()(pooled_conv_c_2)\n",
    "\n",
    "    conv_c_3 = Conv2D(filters = 200,\n",
    "                    kernel_size = (3,100),\n",
    "                    activation = 'relu',\n",
    "                    strides=(1,1),\n",
    "                    name=\"BranchC3\"\n",
    "                    )(conv_c_rs)\n",
    "\n",
    "    pooled_conv_c_3 = MaxPooling2D(pool_size=(conv_c_3.shape[1], 1), padding='valid')(conv_c_3)\n",
    "#     pooled_conv_c_3 = GlobalMaxPooling2D()(conv_c_3)\n",
    "    pooled_conv_c_3 = Flatten()(pooled_conv_c_3)\n",
    "    \n",
    "    C = Concatenate(axis=-1)([pooled_conv_c_1,pooled_conv_c_2])\n",
    "    C = Concatenate(axis=-1)([C,pooled_conv_c_3])\n",
    "\n",
    "    conv_concat = Concatenate(axis=-1)([A,B])\n",
    "    conv_concat = Concatenate(axis=-1)([conv_concat,C])\n",
    "\n",
    "    link_model = Model(inputs = [text_in], outputs = [conv_concat], name = 'DC-CNN_Model')\n",
    "\n",
    "#     issue_model.summary()\n",
    "    return link_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6339a9",
   "metadata": {},
   "source": [
    "### Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50ea5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    text_in_a = Input(shape = (MAX_SEQUENCE_LENGTH,), dtype='int32', name=\"Text_issue_a\")\n",
    "    text_in_b = Input(shape = (MAX_SEQUENCE_LENGTH,), dtype='int32', name=\"Text_issue_b\")\n",
    "    \n",
    "    text_out_a = text_embedding([text_in_a])\n",
    "    text_out_b = text_embedding([text_in_b])\n",
    "    \n",
    "    text_out_a = Reshape((MAX_SEQUENCE_LENGTH,20,1))(text_out_a)\n",
    "    text_out_b = Reshape((MAX_SEQUENCE_LENGTH,20,1))(text_out_b)\n",
    "\n",
    "    link = Concatenate(axis=-1)([text_out_a, text_out_b])\n",
    "    \n",
    "    print(link.shape)\n",
    "    \n",
    "    link_model = DCCNN_Model()\n",
    "    \n",
    "    merged_vector = link_model([link])\n",
    "\n",
    "    concat = Dropout(0.5)(merged_vector)\n",
    "    concat = Dense(units = 512, \n",
    "                activation = 'relu',\n",
    "                )(concat)\n",
    "    concat = BatchNormalization(axis=-1)(concat)\n",
    "    \n",
    "    concat = Dropout(0.5)(concat)\n",
    "    concat = Dense(units = 256, \n",
    "                activation = 'relu',\n",
    "                )(concat)\n",
    "    concat = BatchNormalization(axis=-1)(concat)\n",
    "    \n",
    "    concat = Dropout(0.5)(concat)\n",
    "    concat = Dense(units = 128, \n",
    "                activation = 'relu',\n",
    "                )(concat)\n",
    "    concat = BatchNormalization(axis=-1)(concat)\n",
    "    \n",
    "    concat = Dropout(0.5)(concat)\n",
    "    concat = Dense(units = 64, \n",
    "                activation = 'relu',\n",
    "                )(concat)\n",
    "    concat = BatchNormalization(axis=-1)(concat)\n",
    "       \n",
    "    model = Model(inputs=[text_in_a, text_in_b], outputs=concat)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44afb575",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d2196b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98529\n",
      "24633\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(all_data, test_size = 0.2, random_state = 9)\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "409b7340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset):\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=2)\n",
    "    \n",
    "    class_weight = compute_class_weight(\n",
    "                      class_weight='balanced',\n",
    "                      classes=range(len(id_to_category)),\n",
    "                      y=dataset['label']\n",
    "                    )\n",
    "\n",
    "    model = get_model()\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "#                       Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0), \n",
    "                      loss=tfa.losses.TripletSemiHardLoss())\n",
    "\n",
    "    train_issue_1 = dataset['text_emb_1']\n",
    "    train_issue_1 = np.array(train_issue_1.values.tolist())\n",
    "\n",
    "    train_issue_2 = dataset['text_emb_2']\n",
    "    train_issue_2 = np.array(train_issue_2.values.tolist())\n",
    "    \n",
    "    history = model.fit([train_issue_1, train_issue_2], \n",
    "                        y=dataset['label'], \n",
    "                            callbacks=[callback], \n",
    "                            validation_split=0.1, \n",
    "#                         class_weight = dict(enumerate(class_weight)), \n",
    "                        batch_size=128, epochs=64, verbose=2)\n",
    "    \n",
    "    plot_history(history)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4562f9e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 297, 20, 2)\n",
      "WARNING:tensorflow:From /export/home/lueders/LYNX-venv/lib/python3.8/site-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train on 88676 samples, validate on 9853 samples\n",
      "Epoch 1/64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 10:44:06.564320: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-24 10:44:07.119411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22841 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:3e:00.0, compute capability: 7.5\n",
      "2022-02-24 10:44:10.062331: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8302\n",
      "/export/home/lueders/LYNX-venv/lib/python3.8/site-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88676/88676 - 1062s - loss: 0.8449 - val_loss: 0.7336 - 1062s/epoch - 12ms/sample\n",
      "Epoch 2/64\n",
      "88676/88676 - 1042s - loss: 0.7770 - val_loss: 0.7379 - 1042s/epoch - 12ms/sample\n",
      "Epoch 3/64\n",
      "88676/88676 - 1043s - loss: 0.7431 - val_loss: 0.7261 - 1043s/epoch - 12ms/sample\n",
      "Epoch 4/64\n",
      "88676/88676 - 1042s - loss: 0.7106 - val_loss: 0.6947 - 1042s/epoch - 12ms/sample\n",
      "Epoch 5/64\n",
      "88676/88676 - 1042s - loss: 0.6851 - val_loss: 0.6823 - 1042s/epoch - 12ms/sample\n",
      "Epoch 6/64\n",
      "88676/88676 - 1042s - loss: 0.6595 - val_loss: 0.6816 - 1042s/epoch - 12ms/sample\n",
      "Epoch 7/64\n",
      "88676/88676 - 1042s - loss: 0.6371 - val_loss: 0.6364 - 1042s/epoch - 12ms/sample\n",
      "Epoch 8/64\n",
      "88676/88676 - 1043s - loss: 0.6158 - val_loss: 0.6219 - 1043s/epoch - 12ms/sample\n",
      "Epoch 9/64\n",
      "88676/88676 - 1043s - loss: 0.5997 - val_loss: 0.6170 - 1043s/epoch - 12ms/sample\n",
      "Epoch 10/64\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class_model = train_model(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80315854",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_1_train = train['text_emb_1']\n",
    "issue_1_train = np.array(issue_1_train.values.tolist())\n",
    "\n",
    "issue_2_train = train['text_emb_2']\n",
    "issue_2_train = np.array(issue_2_train.values.tolist())\n",
    "\n",
    "results_train = class_model.predict([issue_1_train, issue_2_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42531ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_1_test = test['text_emb_1']\n",
    "issue_1_test = np.array(issue_1_test.values.tolist())\n",
    "\n",
    "issue_2_test = test['text_emb_2']\n",
    "issue_2_test = np.array(issue_2_test.values.tolist())\n",
    "\n",
    "results_test = class_model.predict([issue_1_test, issue_2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(file='embeddings/train_embed_LT_'+SOURCE+'_DCCNN.npy', arr=results_train)\n",
    "np.save(file='embeddings/test_embed_LT_'+SOURCE+'_DCCNN.npy', arr=results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c3380",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=25, random_state=0)\n",
    "\n",
    "clf.fit(results_train, train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afc98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = clf.predict_proba(results_test)\n",
    "preds = clf.predict(results_test)\n",
    "\n",
    "test['preds_SVM'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666a83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['name', 'linktype', 'issue_id_1', 'issue_id_2', 'mappedtype', 'label']].to_csv('embeddings/train_df_LT'+SOURCE+'_DCCNN.csv')\n",
    "test[['name', 'linktype', 'issue_id_1', 'issue_id_2', 'mappedtype', 'label', 'preds_SVM']].to_csv('embeddings/test_df_LT'+SOURCE+'_DCCNN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34279291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_rep = classification_report(test['label'], test['preds_SVM'], output_dict=True, target_names=category_id_df.mappedtype.to_list())\n",
    "class_rep_df = pd.DataFrame(class_rep).transpose()\n",
    "print(class_rep_df)\n",
    "\n",
    "conf_mat = confusion_matrix(test['preds_SVM'], test['label'])\n",
    "conf_mat_df = pd.DataFrame(conf_mat).transpose()\n",
    "conf_mat_df.rename(index=id_to_category, inplace=True)\n",
    "conf_mat_df.rename(columns=id_to_category, inplace=True)\n",
    "print(conf_mat_df)\n",
    "\n",
    "class_rep_df.to_csv('results/class_rep_LT_'+SOURCE+\"_DCCNN.csv\")\n",
    "conf_mat_df.to_csv('results/conf_mat_LT_'+SOURCE+\"_DCCNN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f091fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
