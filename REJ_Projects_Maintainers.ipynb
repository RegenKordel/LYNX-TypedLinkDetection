{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fb23402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/informatik2/mobis/home/lueders/Desktop/Forschung/Research/DISS/SotP'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "## Ignore warnings\n",
    "import warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "# np.warnings.filterwarnings('ignore')\n",
    "\n",
    "os.chdir('..')\n",
    "from maps.maps import fine_linktype_map\n",
    "from maps.maps import status_map\n",
    "from maps.maps import resolution_map\n",
    "os.chdir('/informatik2/mobis/home/lueders/Desktop/Forschung/Research/DISS/SotP')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3156cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCES = ['Apache', 'Hyperledger', 'IntelDAOS', 'JFrog', 'Jira', \n",
    "           'JiraEcosystem', 'MariaDB', 'Mindville', 'Mojang', 'MongoDB', \n",
    "           'Qt', 'RedHat', 'Sakai', 'SecondLife', 'Sonatype', 'Spring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f85d8116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cleaned_links(source):\n",
    "    #Loading Issues\n",
    "    filename = '../data/cleaned_links_'+source.lower()+'.csv'\n",
    "    link_df = pd.read_csv(filename, encoding=\"UTF-8\", low_memory=False, sep=';')\n",
    "    link_df['maptype'] = link_df['linktype'].map(fine_linktype_map)\n",
    "    \n",
    "    return link_df\n",
    "\n",
    "def load_detailed_issues(source):\n",
    "    #Loading Issues\n",
    "    filename = '../data/detailed_issues_'+source.lower()+'.csv'\n",
    "    issue_df = pd.read_csv(filename, encoding=\"UTF-8\", low_memory=False, sep=',')\n",
    "    \n",
    "    return issue_df\n",
    "\n",
    "def load_link_hist(source):\n",
    "    #Loading Links\n",
    "    filename =  '../data/hist_link_'+source.lower()+'.csv'\n",
    "    link_df = pd.read_csv(filename, encoding=\"UTF-8\", low_memory=False, sep=',')\n",
    "    \n",
    "    # IntelDAOS Epic\n",
    "    # Jira Epicv\n",
    "    # MariaDB Parent Link\n",
    "    # Mojang Linked\n",
    "    # RedHat User Story, Parent Link\n",
    "    if source == 'IntelDAOS' or source == 'Jira':\n",
    "        link_df = link_df[link_df['typeofchange']!='Epic']\n",
    "    elif source == 'MariaDB':\n",
    "        link_df = link_df[link_df['typeofchange']!='Parent Link']\n",
    "    elif source == 'Mojang':\n",
    "        link_df = link_df[link_df['typeofchange']!='Linked']\n",
    "    elif source == 'RedHat':\n",
    "        link_df = link_df[link_df['typeofchange']!='Parent Link']\n",
    "        link_df = link_df[link_df['typeofchange']!='User Story']\n",
    "        \n",
    "    if link_df['maintainer'].isna().values.any():\n",
    "        link_df['maintainer'] = link_df['displayName']\n",
    "    link_df['maintainer'] = link_df['maintainer'].astype('string')\n",
    "    \n",
    "    return link_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8582cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APACHE\n"
     ]
    }
   ],
   "source": [
    "link_dict = {}\n",
    "issue_dict = {}\n",
    "link_hist_dict = {}\n",
    "\n",
    "for s in SOURCES:\n",
    "    print(s.upper())\n",
    "    \n",
    "    links = load_cleaned_links(s)\n",
    "    link_dict[s] = links\n",
    "    \n",
    "    link_hist = load_link_hist(s)\n",
    "    link_hist_dict[s] = link_hist\n",
    "    \n",
    "    issues = load_detailed_issues(s)\n",
    "    issue_dict[s] = issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270bf17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weird = []\n",
    "\n",
    "for s in SOURCES:\n",
    "    try:\n",
    "        issues = issue_dict[s]\n",
    "        try:\n",
    "            issues['update'] = pd.to_datetime(issues['updated'], format='%Y-%m-%d')\n",
    "        except:\n",
    "            issues['updated2'] = issues['updated'].apply(lambda x: x[:10])\n",
    "            try:\n",
    "                issues['update'] = pd.to_datetime(issues['updated2'], format='%Y-%m-%d')\n",
    "            except:\n",
    "                issues['update'] = pd.to_datetime(issues['updated2'], format='%Y-%m-%d', errors='coerce')\n",
    "                issues = issues[issues['update'].notna()]\n",
    "        issues['year'] = issues['update'].dt.year\n",
    "        issues['date'] = issues['update'].dt.date\n",
    "    except:\n",
    "        print(s + \" why no work\")\n",
    "        weird.append(s)\n",
    "    issue_dict[s] = issues\n",
    "    \n",
    "for s in weird:\n",
    "    issues = issue_dict[s]\n",
    "    issues['updated2'] = issues['updated'].apply(lambda x: x[:10])\n",
    "    issues['update'] = pd.to_datetime(issues['updated2'], format='%Y-%m-%d', errors='coerce')\n",
    "    issues['year'] = issues['update'].dt.year\n",
    "    issues['date'] = issues['update'].dt.date\n",
    "    issue_dict[s] = issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weird = []\n",
    "\n",
    "for s in SOURCES:\n",
    "    try:\n",
    "        issues = link_hist_dict[s]\n",
    "        try:\n",
    "            issues['update'] = pd.to_datetime(issues['timestamp'], format='%Y-%m-%d')\n",
    "        except:\n",
    "            issues['updated2'] = issues['timestamp'].apply(lambda x: x[:10])\n",
    "            try:\n",
    "                issues['update'] = pd.to_datetime(issues['updated2'], format='%Y-%m-%d')\n",
    "            except:\n",
    "                issues['update'] = pd.to_datetime(issues['updated2'], format='%Y-%m-%d', errors='coerce')\n",
    "                issues = issues[issues['update'].notna()]\n",
    "        issues['year'] = issues['update'].dt.year\n",
    "        issues['date'] = issues['update'].dt.date\n",
    "    except:\n",
    "        print(s + \" why no work\")\n",
    "        weird.append(s)\n",
    "    link_hist_dict[s] = issues\n",
    "    \n",
    "for s in weird:\n",
    "    issues = link_hist_dict[s]\n",
    "    issues['updated2'] = issues['timestamp'].apply(lambda x: x[:10])\n",
    "    issues['update'] = pd.to_datetime(issues['updated2'], format='%Y-%m-%d', errors='coerce')\n",
    "    issues['year'] = issues['update'].dt.year\n",
    "    issues['date'] = issues['update'].dt.date\n",
    "    link_hist_dict[s] = issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68226d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_activity = pd.DataFrame(columns=['Repository', 'Project', 'last year active'])\n",
    "\n",
    "for s in SOURCES:\n",
    "    temp = issue_dict[s]\n",
    "    for p in temp.projectid.unique():\n",
    "        project_activity.loc[len(project_activity)] = [s, p, temp[temp['projectid']==p]['year'].max()]\n",
    "\n",
    "maintainer_activity = pd.DataFrame(columns=['Repository', 'Maintainer', 'last year active'])\n",
    "\n",
    "for s in SOURCES:\n",
    "    temp = link_hist_dict[s]\n",
    "    for p in temp.maintainer.unique():\n",
    "        maintainer_activity.loc[len(maintainer_activity)] = [s, p, temp[temp['maintainer']==p]['year'].max()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b78e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_activity['last year active'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d12ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "848/project_activity['last year active'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ab6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "maintainer_activity['last year active'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "maintainer_all = {}\n",
    "maintainer_5 = {}\n",
    "maintainer_2 = {}\n",
    "project_all = {}\n",
    "project_5 = {}\n",
    "project_2 = {}\n",
    "\n",
    "mp_overview = pd.DataFrame(columns=['Repository', 'Total Projects', 'Projects5', 'Projects2', \n",
    "                                    'Total Maintainers', 'Maintainer5', 'Maintainer2'])\n",
    "\n",
    "for s in SOURCES:\n",
    "    print(s)\n",
    "    temp_m = maintainer_activity[maintainer_activity['Repository']==s]\n",
    "    maintainer_all[s] = set(temp_m['Maintainer'].values)\n",
    "    temp_p = project_activity[project_activity['Repository']==s]\n",
    "    project_all[s] = set(temp_p['Project'].values)\n",
    "    x5 = temp_p['last year active'].max()-5\n",
    "    x2 = temp_p['last year active'].max()-2\n",
    "    temp_m5 = temp_m[temp_m['last year active']>=x5]\n",
    "    temp_m2 = temp_m[temp_m['last year active']>=x2]\n",
    "    temp_p5 = temp_p[temp_p['last year active']>=x5]\n",
    "    temp_p2 = temp_p[temp_p['last year active']>=x2]\n",
    "    maintainer_5[s] = set(temp_m5['Maintainer'].values)\n",
    "    maintainer_2[s] = set(temp_m2['Maintainer'].values)\n",
    "    project_5[s] = set(temp_p5['Project'].values)\n",
    "    project_2[s] = set(temp_p2['Project'].values)\n",
    "    \n",
    "    mp_overview.loc[len(mp_overview)] = [s, len(temp_p), len(temp_p5), len(temp_p2),\n",
    "                                         len(temp_m),len(temp_m5), len(temp_m2)]\n",
    "mp_overview.to_csv('properties/mp_overview.csv', encoding='utf-8', index=True, sep=',')\n",
    "\n",
    "mp_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d618e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mp_overview.set_index('Repository').to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721015d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_overview.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9782860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for s in SOURCES:\n",
    "    print(s)\n",
    "    lh = link_hist_dict[s]\n",
    "    lh.fillna(\" \", inplace=True)\n",
    "    lh.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    lh['to_1_len'] = lh['to'].apply(lambda x: len(str(x).split()))\n",
    "    lh['from_1_len'] = lh['from'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    lh['to_2_len'] = lh['other_issue_to'].apply(lambda x: len(str(x).split()))\n",
    "    lh['from_2_len'] = lh['other_issue_from'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    lh['to_len'] = lh['to_1_len'] + lh['to_2_len']\n",
    "    lh['from_len'] = lh['from_1_len'] + lh['from_2_len']\n",
    "\n",
    "    lh['total_len'] = lh['to_len'] + lh['from_len']\n",
    "\n",
    "    lh.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    print(len(lh))\n",
    "\n",
    "    other_issues = []\n",
    "    for i in lh.index:\n",
    "        if lh['to_1_len'].iloc[i]==1:\n",
    "            other_issues.append(lh['to'].iloc[i])\n",
    "        elif lh['from_1_len'].iloc[i]==1:\n",
    "            other_issues.append(lh['from'].iloc[i])\n",
    "        elif lh['to_2_len'].iloc[i]==1:\n",
    "            other_issues.append(lh['other_issue_to'].iloc[i])\n",
    "        elif lh['from_2_len'].iloc[i]==1:\n",
    "            other_issues.append(lh['other_issue_from'].iloc[i])\n",
    "        elif lh['to_len'].iloc[i]>0:\n",
    "            match = re.search('[A-Z]+-[0-9]+', str(lh['to'].iloc[i]))\n",
    "            other_issues.append(lh['to'].iloc[i][match.start():match.end()])\n",
    "        elif lh['from_len'].iloc[i]>0:\n",
    "            match = re.search('[A-Z]+-[0-9]+', str(lh['from'].iloc[i]))\n",
    "            other_issues.append(lh['to'].iloc[i][match.start():match.end()])\n",
    "        else: \n",
    "            print(lh.iloc[i])\n",
    "    \n",
    "    lh['other_issue_id'] = other_issues\n",
    "    \n",
    "    link_hist_dict[s] = lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f86d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maintainer_teams = {\n",
    "    'Repo': SOURCES\n",
    "}\n",
    "\n",
    "overview = pd.DataFrame(maintainer_teams)\n",
    "\n",
    "for pre in [0]:\n",
    "    num_components = []\n",
    "    for s in SOURCES:\n",
    "#         print(s)\n",
    "        temp = link_hist_dict_2[s]\n",
    "        \n",
    "        project_df = issue_dict[s][['issue_id', 'projectid']]\n",
    "\n",
    "        lh_plus = temp.merge(project_df, left_on='issue_id', right_on='issue_id')\n",
    "        lh_plus = lh_plus.merge(project_df, left_on='other_issue_id', right_on='issue_id', suffixes=('_iid', '_oid'))\n",
    "        \n",
    "        lh_plus['same_project'] = lh_plus['projectid_iid'] == lh_plus['projectid_oid']\n",
    "        lh_plus = lh_plus[lh_plus['same_project']==False]\n",
    "        \n",
    "        stat_changes = (lh_plus[['projectid_iid', 'projectid_oid']].value_counts()>=len(lh_plus)*pre).reset_index(name='valid')\n",
    "        stat_changes = stat_changes[stat_changes['valid']==True]\n",
    "        edge_weights = (lh_plus[['projectid_iid', 'projectid_oid']].value_counts()/lh_plus[['projectid_iid', 'projectid_oid']].value_counts().sum()*25)[0:len(stat_changes)].to_frame().reset_index()\n",
    "\n",
    "        nodes = set(stat_changes['projectid_iid'].values).union(set(stat_changes['projectid_oid']))                                 \n",
    "\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(nodes)\n",
    "\n",
    "        ew = []\n",
    "        for i in edge_weights.index:\n",
    "            from_node = edge_weights.iloc[i]['projectid_iid']\n",
    "            to_node = edge_weights.iloc[i]['projectid_oid']\n",
    "            weight = edge_weights.iloc[i][0]\n",
    "            ew.append(weight)\n",
    "            G.add_edge(from_node, to_node, weight=weight)\n",
    "\n",
    "        num_comps = len([len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)])\n",
    "        num_components.append(num_comps)\n",
    "        comps = [len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)]\n",
    "    #         components.append(comps)\n",
    "        \n",
    "        pos = nx.circular_layout(G)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "        nx.draw_networkx_edges(G, pos, alpha=1, width=ew, edge_color=\"black\")\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=\"#BB00AA\", alpha=1)\n",
    "        label_options = {\"ec\": \"k\", \"fc\": \"white\", \"alpha\": 0.7}\n",
    "        nx.draw_networkx_labels(G, pos, font_size=14, bbox=label_options)\n",
    "        # nx.draw_networkx(nx.planar_layout(G))\n",
    "\n",
    "        font = {\"fontname\": \"Helvetica\", \"color\": \"k\", \"fontweight\": \"bold\", \"fontsize\": 14}\n",
    "        ax.set_title(s+ \" Projects\", font)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    overview['comps_'+str(pre)] = num_components\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb3400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview.to_csv('properties/project_overview.csv', encoding='utf-8', index=True, sep=',')\n",
    "\n",
    "overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a05c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_hist_dict_5 = {}\n",
    "link_hist_dict_2 = {}\n",
    "\n",
    "for s in SOURCES:\n",
    "    print(s)\n",
    "    temp_df = link_hist_dict[s]\n",
    "    temp_p = project_activity[project_activity['Repository']==s]\n",
    "    \n",
    "    temp_5_df = temp_df[temp_df['year']>=temp_p['last year active'].max()-5]\n",
    "    temp_2_df = temp_df[temp_df['year']>=temp_p['last year active'].max()-2]\n",
    "    \n",
    "    link_hist_dict_5[s] = temp_5_df\n",
    "    link_hist_dict_2[s] = temp_2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b19cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_percentage(temp_df, col, perc):\n",
    "    total = len(temp_df)\n",
    "    for i in np.linspace(0, 0.3, num=1001):\n",
    "        k = int(np.round(i*len(temp_df),0))\n",
    "        x = temp_df[col].value_counts().sort_values(ascending=False)[0:k].sum()/total\n",
    "        m = (temp_df[col].value_counts().sort_values(ascending=False)[0:k].index)\n",
    "        if (x >= perc):\n",
    "            return i, x, m\n",
    "\n",
    "maintainers_dict = {}\n",
    "        \n",
    "for s in SOURCES:\n",
    "    print(s)\n",
    "    temp_df = link_hist_dict_2[s]\n",
    "    \n",
    "    i, x, m = find_percentage(temp_df, 'maintainer', 0.9)\n",
    "    print([i, x, len(m)])\n",
    "    maintainers_dict[s] = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e30d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "maintainer_project_dict = {}\n",
    "mp_df_dict = {}\n",
    "\n",
    "for s in SOURCES:\n",
    "    print(s)\n",
    "    temp_df = link_hist_dict_2[s]\n",
    "    temp_df['other_issue_to'].fillna('', inplace=True)\n",
    "    temp_df['other_issue_from'].fillna('', inplace=True)\n",
    "    temp_df['other_issue_id'] = temp_df['other_issue_to']+temp_df['other_issue_from']\n",
    "    temp_df['project_1'] = temp_df['issue_id'].str.extract(r'(\\D+)')\n",
    "    temp_df['project_2'] = temp_df['other_issue_id'].str.extract(r'(\\D+)')\n",
    "    valid_maintainers = maintainers_dict[s]\n",
    "    temp_df = temp_df[temp_df['maintainer'].isin(valid_maintainers)]\n",
    "    temp_df_1 = temp_df[['maintainer', 'project_1']]\n",
    "    temp_df_1.columns = ['maintainer', 'project']\n",
    "    temp_df_2 = temp_df[['maintainer', 'project_2']]\n",
    "    temp_df_2.columns = ['maintainer', 'project']\n",
    "    temp_df_t = pd.concat([temp_df_1, temp_df_2], ignore_index=True)\n",
    "    \n",
    "    mp_df_dict[s] = temp_df_t\n",
    "    \n",
    "    temp_p = pd.crosstab(index=temp_df_t['maintainer'], columns=temp_df_t['project'])\n",
    "    temp_p.fillna(0, inplace=True)\n",
    "    print(len(temp_p)==len(valid_maintainers))\n",
    "    \n",
    "    maintainer_project_dict[s] = temp_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "maintainer_teams = {\n",
    "    'Repo': SOURCES\n",
    "}\n",
    "\n",
    "overview = pd.DataFrame(maintainer_teams)\n",
    "\n",
    "for i in np.linspace(0.05, 0.25, num=5):\n",
    "    num_components = []\n",
    "    for s in SOURCES:\n",
    "        m_p_df = maintainer_project_dict[s]\n",
    "        m_p_per_df = m_p_df.div(m_p_df.sum(axis=1), axis=0)\n",
    "        edges = m_p_per_df[m_p_per_df >= i].stack().index.tolist()\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(m_p_df.index, bipartite=0)\n",
    "        G.add_nodes_from(m_p_df.columns, bipartite=1)\n",
    "        G.add_edges_from(edges)\n",
    "        num_comps = len([len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True) if len(c)>1])\n",
    "        num_components.append(num_comps)\n",
    "        comps = [len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True) if len(c)>1]\n",
    "#         components.append(comps)\n",
    "    overview['comps_'+str(round(i,2))] = num_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview.set_index('Repo', inplace=True)\n",
    "overview.to_csv('properties/maintainer_overview.csv', encoding='utf-8', index=True, sep=',')\n",
    "\n",
    "overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90c5e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "maintainer_teams = {\n",
    "    'Repo': SOURCES\n",
    "}\n",
    "\n",
    "overview = pd.DataFrame(maintainer_teams)\n",
    "\n",
    "for i in np.linspace(0.05, 0.25, num=5):\n",
    "    num_components = []\n",
    "    for s in SOURCES:\n",
    "        temp = link_hist_dict[s]\n",
    "        project_df = issue_dict[s][['issue_id', 'projectid']]\n",
    "\n",
    "        lh_plus = temp.merge(project_df, left_on='issue_id', right_on='issue_id')\n",
    "        lh_plus = lh_plus.merge(project_df, left_on='other_issue_id', right_on='issue_id', suffixes=('_iid', '_oid'))\n",
    "\n",
    "        temp = lh_plus[['projectid_iid', 'projectid_oid']].value_counts()\n",
    "        edges = m_p_per_df[m_p_per_df >= i].stack().index.tolist()\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(temp.index)\n",
    "        G.add_edges_from(edges)\n",
    "        num_comps = len([len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True) if len(c)>1])\n",
    "        num_components.append(num_comps)\n",
    "        comps = [len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True) if len(c)>1]\n",
    "#         components.append(comps)\n",
    "    overview['comps_'+str(round(i,2))] = num_components\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fcd57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866691bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
